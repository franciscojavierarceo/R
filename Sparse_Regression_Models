#===========================================================================================
# Importing required packages
#===========================================================================================
require(ggplot2)
require(scales)
require(Matrix)
require(glmnet)
require(sqldf)
require(reshape)
require(gridExtra)
#===========================================================================================
# Author: Francisco Javier Arceo
# Date: March 25th 2013
# Contact: francisco.arceo@aig.com
#===============
# Purpose: The code below shows a LASSO regression model and an OLS regression model.
# This is an exposition of a high dimensional sparse regression model and its advantage.
# In this Monte Carlo, you'll notice that GLMNET (i.e., LASSO) performs much, much better
# than a classical Ordinary Least-Squares regression model.
# The only dependencies this code has are on the packages listed above, so long as these
# packages are installed, this code will execute without any issues. 
#===============
# Academic Paper on the Elastic Net: www.stanford.edu/~hastie/Papers/glmnet.pdf
#===========================================================================================
# This section creates the data
set.seed(4985)
n <- 1e4
p1 <- 1:(n/4)
p2 <- (n/4+1):(n/2)
p3 <- (n/2+1):(3*n/4)
p4 <- (3*n/4+1):n
summary(p1);summary(p2);summary(p3);summary(p4)
lets       <- rep('',n)
lets[p1]   <- sample(paste(letters,sep=''),length(p1),replace=T)
lets[p2]   <- sample(paste(letters,letters,sep=''),length(p2),replace=T)
lets[p3]   <- sample(paste(letters,letters,letters,sep=''),length(p3),replace=T)
lets[p4]   <- sample(paste(letters,letters,letters,letters,sep=''),length(p4),replace=T)
lets <- paste(lets,sort(lets))[1:n]
length(unique(lets))
df <- data.frame(age=rnorm(n),geo=factor(lets))
#===========================================================================================
# The line below creates a sparse matrix from the j levels in the dataframe df and interacts
# those categories with the continuous age variable to create all of the proper interaction
# terms...this is a nice feature because you can trivially create categorical and continuous
# interactions using the sparse.model.matrix() command.
#===============
xs <- sparse.model.matrix(~age+geo+age*geo-1,data=df)     
#===============
print(dim(xs))                      # The second number shows the number of columns
print(xs[1:20,1:4])                 # What the first 20 rows and 4 columns look like of a sparse matrix
betas <- rnorm(dim(xs)[2])          # This creates the model coefficients
yhat  <- xs%*% betas                # This creates the model prediction
e     <- rnorm(n)                   # This is the error term
ydep  <- yhat + e                   # This creates the actual dependent variable
ydep  <- ydep + abs(min(ydep))      # Renormalizing to make the dependent non-negative
df$ydep <- array(ydep)
df$yhat <- array(yhat)
# This is a plot of the actual versus predicted
ggplot(df,aes(x=yhat,y=ydep))+geom_smooth(colour='red',method='lm')+
  geom_point(colour='blue')+theme_bw()
# Notice that the best possible model prediction is ~65% of the variance
#===========================================================================================
summary(as.numeric(ydep))                         # Summary statistics of the dependent
ggplot(data.frame(ydep=as.numeric(ydep)),         # Histogram of the dependent
       aes(x=ydep))+geom_histogram(fill='blue',
       binwidth=1)+theme_bw()+xlab('Dependent Variable')+
  ylab("Count")
#===============
trainfilt <- ifelse(runif(n)<=0.7,1,0)            # Random Training split
#===========================================================================================
# Running the regression models
#===============
# Below is the LASSO/Regularized regression model
# This is nice because it solves (X'X)^-1 quickly
# by using sparse matrices instead of typical (dense)
# matrices.
#===============
glmnet_mod <- cv.glmnet(x=xs[trainfilt==1,],      # Specifying sparse input matrix
                        y=ydep[trainfilt==1],     # Specifying dependent variable
                        alpha  = 1.0,             # Alpha=1 -> Lasso; Alpha=0 -> Ridge
                        type.measure='mse',       # Mean Squared Error
                        nlambda= 100,             # Number of lambda values to try
                        nfolds=10)                # Number of crossfold validations
#===============
# Below is the classical OLS model
# Specifying a lambda of 0 omits wipes out 
# lambda parameter from the minimization 
#===============
ols_mod <- glmnet(x= xs[trainfilt==1,],           
                        y=ydep[trainfilt==1],     
                        alpha  = 0,
                        lambda=0)
fllglmnet_mod <- glmnet(x= xs[trainfilt==1,],
                        y=ydep[trainfilt==1],     
                        alpha  = 0,
                        nlambda=50)
fllglmnet_mod2 <- glmnet(x= xs[trainfilt==1,],
                        y=ydep[trainfilt==1],     
                        alpha  = 1,
                        nlambda=50)
#===============
betas <- cbind2(coef(glmnet_mod,s='lambda.min'),  # This extracts the coefficients
                coef(ols_mod))
betas <- data.frame(as.matrix(betas))             # This changes the matrices into dataframes
names(betas) <- c('Glmnet_Betas','OLS_Betas')     # This renames the columns
# Notice how many of the coefficients are shrunk to zero
with(betas,plot(OLS_Betas,Glmnet_Betas,pch=16,col=4,
                xlab='OLS Coefficients',
                ylab='LASSO Coefficients',
                main='LASSO vs OLS Coefficients')); grid(5,5,'gray44')
plot(glmnet_mod)
# Let's just look at ten of them
glmnetbs <- fllglmnet_mod$beta[1:10,]
glmnetbs <- data.frame(t(as.matrix(glmnetbs)))
lambdas <- fllglmnet_mod$lambda
tmp <- data.frame(lambdas,glmnetbs)
tmp2<-melt(tmp,'lambdas');names(tmp2)[2] <- 'Variable'
ggplot(tmp2[tmp2$lambdas<40,],aes(x=-lambdas,y=value,colour=Variable))+
  geom_line(size=1.05)+theme_bw()+ggtitle('Ridge Coefficients')+
  theme(legend.position='bottom')+xlab('-Lambda Value')+
  ylab('Coefficient')
# Next
glmnetbs <- fllglmnet_mod2$beta[1:15,]
glmnetbs <- data.frame(t(as.matrix(glmnetbs)))
lambdas <- fllglmnet_mod$lambda
tmp <- data.frame(lambdas,glmnetbs)
tmp2<-melt(tmp,'lambdas');names(tmp2)[2] <- 'Variable'
ggplot(tmp2[tmp2$lambdas<70,],aes(x=-lambdas,y=value,colour=Variable))+
  geom_line(size=1.05)+theme_bw()+ggtitle('LASSO Coefficients')+
  theme(legend.position='bottom')+xlab('-Lambda Value')+
  ylab('Coefficient')
#===========================================================================================
# Some quick functions that help look at model performance
#===============
{
  mygini<-function(pred, actual) {
    w<-rep(1, length(pred))
    v<-data.frame(o=pred, p=pred, a=actual, w=w)
    v<-v[order(v$o),]
    v$cumm_w<-cumsum(v$w)
    v$cumm_y<-cumsum(v$w*v$a)
    total_w<-sum(v$w)
    gini<-with(v,1-2* sum(cumm_y*w)/(sum(a*w)*total_w))
    return(gini)
  }
  mylift<-function(pred, actual, w=NULL, n=10) {     
    require(sqldf);require(reshape);require(glmnet)
    if (length(w)==0) {
      w<-rep(1.0, length(pred))
    }
    pred <- as.numeric(pred)
    v<-data.frame(o=pred, p=pred, a=actual, w=w)
    v<-v[order(v$o),]
    v$cumm_w<-cumsum(v$w)
    v$cumm_y<-cumsum(v$w*v$a)
    total_w<-sum(v$w)
    gini <-with(v,1-2* sum(cumm_y*w)/(sum(a*w)*total_w))
    NormGini <- gini/mygini(actual,actual)
    v$pidx<-round(v$cumm_w*n/total_w+0.5)
    v$pidx[v$pidx>n]<-n
    v$pidx[v$pidx<1]<-1
    sum1<-sqldf("select pidx, 
                  sum(w) as w, 
                  min(o) as min, 
                  max(o) as max, 
                  sum(o*w)/sum(w) as o, 
                  sum(p*w)/sum(w) as p, 
                  sum(a*w)/sum(w) as a, 
                  sum(a*w) as a_cnt 
                  from v group by pidx")
    sum1$err<-with(sum1, a/p)
    sum1$gini <-with(v,1-2* sum(cumm_y*w)/(sum(a*w)*total_w))
    x2 <- melt(sum1[,c('pidx','p','a')],id='pidx')
    names(x2) <- c('Decile','Variable','Value')
    x2$Variable <- as.character(x2$Variable)
    x2$Variable[x2$Variable=='a'] <- 'Actual'
    x2$Variable[x2$Variable=='p'] <- 'Predicted'
    main_title <- paste('Deciles Sorted by Prediction \n',NormGini)
    plt <- ggplot(x2)+geom_line(aes(x=Decile,y=Value,colour=Variable,linetype=Variable),size=0.75)+
      geom_point(aes(x=Decile,y=Value,colour=Variable),size=4)+
      theme_bw()+scale_linetype_manual(values=c("longdash", "solid"))+
      scale_colour_manual(values=c("blue","red"))+
      theme(legend.position='bottom',plot.title=element_text(size=10))+
      scale_x_continuous(labels=comma,breaks=1:n)+scale_y_continuous(labels=dollar)+
      ggtitle(main_title)+ylab('Actual and Realized')
    return(plt)
  }
  mape <- function(pred,actual){ # Mean Absolute Percent Error
    mape <- sum(abs(pred-actual)/length(actual))/length(actual)
    print(paste("The Mean Absolute Percent Error is: ",round(mape*100,2),"%",sep=''))
  }
  rmse <- function(pred,actual){ # Root Mean-Squared Error
    rmse <- round(sqrt(sum( (pred - actual)^2 )/length(actual)),4)
    print(paste("The Root Mean-Squared Error is:",rmse))
  }
  prsq <- function(pred,actual){ # Psuedo R-squared
    prsq <- round(summary(lm(actual~pred-1))$r.squared,4)
    paste('The Psuedo R-Squared is:',prsq)
  }
}
#===========================================================================================
# The model predictions
#===============
enet_pred <- predict(glmnet_mod,newx=xs,s='lambda.min')
ols_pred  <- predict(ols_mod,newx=xs,s='lambda.min')
#===============
# The residuals (Y_i - E[Y|X])
#===============
enet_res <- array(ydep - enet_pred);  enet_res  <- enet_res[trainfilt==0]
ols_res  <- array(ydep - ols_pred);    ols_res   <- ols_res[trainfilt==0]
#===========================================================================================
# Lower values are better - note: filtered on the hold out
#===============
mape(ols_pred[trainfilt==0],ydep[trainfilt==0])
mape(enet_pred[trainfilt==0],ydep[trainfilt==0])
rmse(ols_pred[trainfilt==0],ydep[trainfilt==0])
rmse(enet_pred[trainfilt==0],ydep[trainfilt==0])
prsq(ols_pred[trainfilt==0],ydep[trainfilt==0])
prsq(enet_pred[trainfilt==0],ydep[trainfilt==0])
#===========================================================================================
# Lift Charts that also include gini -- note the GINI gives a counterintuitive result here
# but the other model fit statistics suggest LASSO is strictly superior
#===============
mylift(ols_pred[trainfilt==0],ydep[trainfilt==0])+xlab('Deciles - OLS Model (Hold Out)')
mylift(enet_pred[trainfilt==0],ydep[trainfilt==0])+xlab('Deciles - LASSO Model (Hold Out)')
#===========================================================================================
# Now you can look at the individual predictions
#===============
xlimits <- c(min(enet_pred),max(enet_pred))
df1 <- data.frame(ACTUAL=ydep[trainfilt==0],
                  PRED=ols_pred[trainfilt==0],
                  type='OLS')
df2<- data.frame(ACTUAL=ydep[trainfilt==0],
                 PRED=enet_pred[trainfilt==0],
                 type='LASSO')
df <- rbind(df1,df2); df1 <- NULL; df2 <- NULL
#===============
# Below generates a scatter plot of the actual and predicted for each model
ggplot(df,aes(x=PRED,y=ACTUAL,colour=type)) + geom_point()+theme_bw()+
  stat_smooth(method='lm')+scale_x_continuous(limits=xlimits)+
  theme(legend.position='bottom')+xlab('Predicted')+ylab('Actual')
# Note that I'm overlaying the actuals twice but 
# in most cases they have different predictions 
# Ignore the warning messaged, it's because I am intentionally limiting the domain
#===========================================================================================
# Below are two histograms of the hold out error
#===============
xlimits <- c(min(ols_res,enet_res),max(ols_res,enet_res))
plt1 <- ggplot(data.frame(ols_res),aes(x=ols_res))+
  geom_histogram(fill='blue',binwidth=1)+theme_bw()+
  ggtitle('Histogram of OLS Residuals \n On Hold Out')+
  scale_x_continuous(limits=xlimits)
plt2 <- ggplot(data.frame(enet_res),aes(x=enet_res))+
  geom_histogram(fill='blue',binwidth=1)+theme_bw()+
  ggtitle('Histogram of the Elastic Net Residuals \n On Hold Out')+
  scale_x_continuous(limits=xlimits)
grid.arrange(plt1,plt2)
# Notice how the range of the OLS residuals is much larger [-54,65] 
# than the range of the LASSO residuals [-7,6]
#===========================================================================================
