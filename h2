#=====================================================================
# Author: Francisco Javier Arceo
# Purpose: Homework 2 - Machine Learning for Data Science
# Date: February 24 2015
#=====================================================================
# To load the .mat file
require('R.matlab')
#=====================================================================
setwd('/Users/franciscojavierarceo/School/ML/2 - Homework/HMWK2/')
#=====================================================================
MatLabData <- readMat('mnist_mat.mat')      # Reading the data
xstrn <- MatLabData$Xtrain                  # Training data
ystrn <- MatLabData$label.train             # Training data
xstst <- MatLabData$Xtest                   # Test data
ystst <- MatLabData$label.test              # Test data
Q <- MatLabData$Q                           # Matrix of Principal Components
xs <- cbind(xstrn,xstst)                    # Putting the data together for KNN
outdist <- as.matrix(dist(t(xs)))           # Have to do transpose
TestData <- outdist[1:5000,5001:5500]       # Splitting into the training data
#=======================================================================================================
# Functions
#=======================================================================================================
# Inefficient ot continue to caclulate dist function each time
# so calculate the distance matrix first, then feed in the test data
myknn <- function(TestData,y,k=1){
  PredClass <- rep(NA,length=500)
  for(i in 1:dim(TestData)[2]){
    TrainNeighbors <- data.frame(Neighbor=TestData[,i],
                                 RowIndex=1:length(array(y)),
                                 Labels=array(y))
    TrainNeighbors <- TrainNeighbors[order(TrainNeighbors$Neighbor,decreasing=F),]
    nnout <- data.frame(table(TrainNeighbors$Labels[1:k])); names(nnout)[1] <- 'Label'
    # Majority Vote - choose *first* max in case of ties
    nnoutval <- nnout$Label[which.max(nnout$Freq)]
    PredClass[i] <- as.character(nnoutval)  
  }
  return(PredClass)
}
MyTrace <- function(x){
  n <- dim(x)[1]
  k <- dim(x)[2]
  out <- c()
  for(i in 1:n){
    for(j in 1:k){
      if(i==j){
        out[i] <- x[i,j]
      }
    }
  }
  return(sum(out))
}
rotate <- function(x){ 
  t(apply(x, 2, rev)) 
}
Verify <- function(pred,flt,xs,y,vl=1){
  yhat <- rotate(matrix(Q%*%xs[,flt[vl]],ncol=28,byrow=T))
  image(yhat,,col=hsv(seq(0.25,0,length=256),1,1))
  print(paste('Predicted =', pred[flt[vl]],'actual =',y[flt[vl]]))
}
#=======================================================================================================
PredAccuracy <- c()       # Initializing the prediction accuracy
for(i in 1:5){            # Looping for all k=1,2,3,4,5.
  assign(paste('pred',i,sep=''),myknn(TestData,ystrn,k=i))
  PredAccuracy[i] <- MyTrace(table(get(paste('pred',i,sep='')),ystst))/length(ystst)
}
outdf <- data.frame(K=1:5,PredAccuracy) ; print(outdf)
plot(outdf,type='b',pch=16,ylim=c(0.8,1))
flt1 <- which(ystst!=pred1)
flt3 <- which(ystst!=pred3)
flt5 <- which(ystst!=pred5)
#===============================
# Plots of the errors for KNN
#===============================
par(mfrow=c(3,1))
Verify(pred1,flt1,xstst,ystst,2)
Verify(pred3,flt3,xstst,ystst,3)
Verify(pred5,flt5,xstst,ystst,6)
#=======================================================================================================
# Problem 3b
#=======================================================================================================
# - Implement the Bayes classifier using the Multivariate Gaussian
# - Derive MLE for the 10-d distribution on classes and Gaussian
#    parameters for a particular class j
# - Show the confusion matrix in a table. Show the trace(CM)/500
# - Show th mean of each Gaussian as an image using the provided Q
#    matrix
# - Show three misclassified examples & show the probability
#   distribution on the 10 digits learned by the Bayes classifier for
#   each one.
#=======================================================================================================

#=======================================================================================================
# Problem 3c
#=======================================================================================================
# - Implement the multiclass logistic regression - set the step size
#   on the order of \rho = 0.1/5000
# - For each cycle through w_o,...,w_9 calculate the gradient and 
#   plot as a function of each iteration. Do 1000 iterations
# - Show the confusion matrix in a table with trace(CM)/500
# - Show three misclassified examples & show the probability
#   distribution on the 10 digits learned by the softmax function
#   for each one.
#=======================================================================================================

#=======================================================================================================
# End
#=======================================================================================================
