#=======================================================================================================
# Author: Francisco Javier Arceo
# Purpose: Homework 2 - Machine Learning for Data Science
# Date: February 24 2015
#=======================================================================================================
# To load the .mat file
require('scatterplot3d')
require('R.matlab')
#=======================================================================================================
setwd('/Users/franciscojavierarceo/Data/Hmwk/mnist_csv') # Folder
#=======================================================================================================
MatLabData <- readMat('mnist_mat.mat')      # Reading the data
xstrn <- MatLabData$Xtrain                  # Training data
ystrn <- MatLabData$label.train             # Training data
xstst <- MatLabData$Xtest                   # Test data
ystst <- MatLabData$label.test              # Test data
Q <- MatLabData$Q                           # Matrix of Principal Components
xs <- cbind(xstrn,xstst)                    # Putting the data together for KNN
outdist <- as.matrix(dist(t(xs)))           # Have to do transpose
TestData <- outdist[1:5000,5001:5500]       # Splitting into the training data
#=======================================================================================================
# Functions
#=======================================================================================================
# Inefficient ot continue to caclulate dist function each time
# so calculate the distance matrix first, then feed in the test data
myknn <- function(TestData,y,k=1){
  PredClass <- rep(NA,length=500)
  for(i in 1:dim(TestData)[2]){
    TrainNeighbors <- data.frame(Neighbor=TestData[,i],
                                 RowIndex=1:length(array(y)),
                                 Labels=array(y))
    TrainNeighbors <- TrainNeighbors[order(TrainNeighbors$Neighbor,decreasing=F),]
    nnout <- data.frame(table(TrainNeighbors$Labels[1:k])); names(nnout)[1] <- 'Label'
    # Majority Vote - choose *first* max in case of ties
    nnoutval <- nnout$Label[which.max(nnout$Freq)]
    PredClass[i] <- as.character(nnoutval)  
  }
  return(PredClass)
}
MyTrace <- function(x){
  n <- dim(x)[1]
  k <- dim(x)[2]
  out <- c()
  for(i in 1:n){
    for(j in 1:k){
      if(i==j){
        out[i] <- x[i,j]
      }
    }
  }
  return(sum(out))
}
rotate <- function(x){ 
  t(apply(x, 2, rev)) 
}
Verify <- function(pred,flt,xs,y,vl=1){
  yhat <- rotate(matrix(Q%*%xs[,flt[vl]],ncol=28,byrow=T))
  image(yhat,,col=hsv(seq(0.25,0,length=256),1,1))
  print(paste('Predicted =', pred[flt[vl]],'actual =',y[flt[vl]]))
}
#=======================================================================================================
PredAccuracy <- c()       # Initializing the prediction accuracy
for(i in 1:5){            # Looping for all k=1,2,3,4,5.
  assign(paste('pred',i,sep=''),myknn(TestData,ystrn,k=i))
  PredAccuracy[i] <- MyTrace(table(get(paste('pred',i,sep='')),ystst))/length(ystst)
}
outdf <- data.frame(K=1:5,PredAccuracy) ; print(outdf)
plot(outdf,type='b',pch=16,ylim=c(0.8,1))
flt1 <- which(ystst!=pred1)
flt3 <- which(ystst!=pred3)
flt5 <- which(ystst!=pred5)
#===============================
# Plots of the errors for KNN
#===============================
par(mfrow=c(3,1))
Verify(pred1,flt1,xstst,ystst,2)
Verify(pred3,flt3,xstst,ystst,3)
Verify(pred5,flt5,xstst,ystst,6)
#=======================================================================================================
# Problem 3b
#=======================================================================================================
# - Implement the Bayes classifier using the Multivariate Gaussian
# - Derive MLE for the 10-d distribution on classes and Gaussian
#    parameters for a particular class j
# - Show the confusion matrix in a table. Show the trace(CM)/500
# - Show th mean of each Gaussian as an image using the provided Q
#    matrix
# - Show three misclassified examples & show the probability
#   distribution on the 10 digits learned by the Bayes classifier for
#   each one.
#=======================================================================================================
MatrixBuild <- function(input){
  output <- data.frame(Level0=rep(0,length(input)))
  lvls <- unique(input)
  for(i in 1:length(lvls)){
    tmp <- ifelse(input==lvls[i],1,0)
    output[,paste('Level',i-1,sep='')] <- tmp
  }
  return(output)
}
Mean <- function(x){
  return( sum( x ) / length(x))
}
Stdev <- function(x){ # Sample Standard Deviation
  return( sqrt( sum( (x - Mean(x))^2 )/length(x)-1 ) )
}
correlation <- function(x,y){
  return( covariance(x,y)/(Stdev(x)*Stdev(y)))
}
covariance <- function(x,y){ # Sample Covariance
  return( sum( (x - Mean(x))*(y-Mean(y)) ) / (length(x)-1) )
}
CorMatrix <- function(x){
  k <- dim(x)[2]
  out <- matrix(rep(0,length=k*k),ncol=k,nrow=k)  
  for(i in 1:k){
    for(j in 1:k){
      out[i,j] <- correlation(x[i],x[j])
    }
  }
  return(out)
}
CovMatrix <- function(x){
  k <- dim(x)[2]
  out <- matrix(rep(0,length=k*k),ncol=k,nrow=k)  
  colnames(out) <- colnames(x)
  rownames(out) <- colnames(x)
  for(i in 1:k){
    for(j in 1:k){
      out[i,j] <- covariance(x[,i],x[,j])
    }
  }
  return(out)
}
PriorMean <- function(y){
  out <- c()
  for(j in 1:dim(y)[2]){
    out[j] <- sum(y[,j]) / length(y[,j])
  }
  return(out)
}
CondMean <- function(x,y){
  mu_yk <- c()
  for(j in 1:dim(y)[2]){
    mu_yk[j] <- (1/length(y[,j])) * sum( ifelse(y[,j]==1,1,0)* (x))    
  }
  return(mu_yk)
}
CondVar <- function(x,y,muvec){
  k <- dim(y)[2]
  sigma_yk <- matrix(rep(0,k*k),ncol=k,nrow=k)
  for(j in 1:k){
    tmp <-  sum( ifelse(y[,j]==1,1,0))%*%(t(x- muvec[j]) %*% (x- muvec[j]))
    tmp2 <-  sum( ifelse(y[,j]==1,1,0)) * ((x- muvec) %*% t(x- muvec))
    sigma_yk[j,j] <- tmp
  }
}
CondVar2 <- function(x,y,mnx){
  return(sum( y * (x-mnx)^2 )/ length(x))
}
ColMax <- function(df){
  apply(df, 1, function(x) {max(which(x == max(x, na.rm = TRUE)))})
}
#=======================================================================================================
YMatrix <- MatrixBuild(array(ystrn))
ClassPriors <- PriorMean(YMatrix)
Sigma <- CovMatrix(YMatrix)
ClassCondDensity <- function(muvec,sigma){
  out <- 
}
x1 <- xstrn[1,]
muy <- CondMean(x1,YMatrix)
CondVar(x1,YMatrix,muy)
mu1 <- mean(YMatrix[,1]*x1)
sigma1 <- sum(YMatrix[,1]*((x1 - mu1)*(x1-mu1)))/length(x1)
ClassPriors
# Let's do a small example
ytmp <- YMatrix[,1]
xtmp <- x1
mnvl <- CondMean(xtmp,y)[1]
tmpmat <- data.frame(X1=rep(0,length(xtmp)))
final_out <- data.frame(x1=rep(0,length(xtmp)))
for(j in 1:dim(YMatrix)[2]){
  y <- YMatrix[,j]
  for(i in 1:dim(xstrn)[1]){
    xtmp <- xstrn[i,]
    mnvl <- CondMean(xtmp,y)[1]
    s2 <- CondVar2(xtmp,y[,i],mnvl)
    pred <- (1/ sqrt( 2*pi * s2)) * exp(-(1/2)*((xtmp - mnvl)^2) / s2) 
    tmpmat[,paste('X',i,sep='')]  <- pred
    if(i==1){plot(ecdf(pred),xlim=c(0,3))}
    else {
      lines(ecdf(pred),col=rainbow(10)[i])
    }
  }  
  out <- rep(0,length(tmpmat[,1]))
  for(i in 2:dim(tmpmat)[2]){
    out <- tmpmat[,i]*tmpmat[,i-1]  
  }
  final_out[,paste('prob',j,sep='')] <- out
}
lines(ecdf(out),col='red')
table(y[,1],ifelse(out>=1,1,0))
head(tmpmat)

#=======================================================================================================
# Problem 3c
#=======================================================================================================
# - Implement the multiclass logistic regression - set the step size
#   on the order of \rho = 0.1/5000
# - For each cycle through w_o,...,w_9 calculate the gradient and 
#   plot as a function of each iteration. Do 1000 iterations
# - Show the confusion matrix in a table with trace(CM)/500
# - Show three misclassified examples & show the probability
#   distribution on the 10 digits learned by the softmax function
#   for each one.
#=======================================================================================================
xstrndf <- data.frame(t(xstrn))
print(head(YMatrix))
print(dim(xstrndf))
# Log-Likelihood  # y is a vector and x is a matrix
x <- data.frame(Intercept=1,xstrndf)
x <- as.matrix(x)

MySoftMax <- function(x,y){
  # Initialize
  wvec <- data.frame(Betas1=rep(0,21))
  predmtrx <- data.frame(Pred0=rep(0,dim(x)[1]))
  for(i in 1:dim(y)[2]){
    w <- solve(t(x)%*%x) %*% t(x) %*%y[,i]
    wvec[,paste('Betas',i,sep='')] <-  w
    predmtrx[,paste('Pred',i-1,sep='')] <- exp(x %*% w)
  }
  TotalPred <- rowSums(predmtrx)
  Fullpredmtrx <- predmtrx / TotalPred
  ClassPred <- ColMax(Fullpredmtrx)
  return(ClassPred)
  Iy <- diag(length(y[,i]))
  
}
#=======================================================================================================
# End
#=======================================================================================================
